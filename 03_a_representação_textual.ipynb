{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesluiz/pln-na-pratica/blob/main/03_a_representa%C3%A7%C3%A3o_textual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TfMtJgRJAC2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ca029a-5188-4465-9c61-b8f1106e57f8"
      },
      "source": [
        "# instalação dos pacotes necessários\n",
        "!pip install nltk==3.8.1\n",
        "!pip install gensim==4.3.2\n",
        "!pip install umap-learn==0.5.5\n",
        "!pip install wikipedia==1.4.0\n",
        "!pip install unidecode==1.3.8"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.2)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (6.4.0)\n",
            "Requirement already satisfied: umap-learn==0.5.5 in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (0.5.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.5) (4.66.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn==0.5.5) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn==0.5.5) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn==0.5.5) (3.3.0)\n",
            "Requirement already satisfied: wikipedia==1.4.0 in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.4.0) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.4.0) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n",
            "Requirement already satisfied: unidecode==1.3.8 in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6V8u14WAC26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60c51f5-596b-4d09-c05b-b4cb71083024"
      },
      "source": [
        "# Importações da biblioteca padrão\n",
        "import bz2\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import urllib.request\n",
        "import warnings\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Importações de bibliotecas de terceiros\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import umap\n",
        "import wikipedia\n",
        "from gensim.models import word2vec\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.tokenize import sent_tokenize\n",
        "#from nltk.util import ngrams\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.manifold import TSNE\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Downloads do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Configurações e comandos específicos (por exemplo, desativar avisos)\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Pacotes importados com sucesso; notebook pronto para uso!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pacotes importados com sucesso; notebook pronto para uso!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# declara as funções utilitárias do notebook\n",
        "def formata_msg(nivel, msg):\n",
        "  \"\"\"\n",
        "    Formata uma mensagem de log incluindo o nível de severidade, timestamp\n",
        "    e a mensagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - nivel (str): Nível de severidade da mensagem (ex: 'INFO', 'ERROR', 'WARNING').\n",
        "    - msg (str): A mensagem de log propriamente dita.\n",
        "\n",
        "    Retorna:\n",
        "    - str: A mensagem de log formatada.\n",
        "  \"\"\"\n",
        "\n",
        "  timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "  return f\"[{nivel}] {timestamp} - {msg}\"\n",
        "\n",
        "\n",
        "def baixar_extrair(modelo, arquitetura):\n",
        "    \"\"\"\n",
        "    Baixa e extrai um arquivo zip de embeddings de um modelo e arquitetura\n",
        "    específicos de um servidor remoto.\n",
        "\n",
        "    Args:\n",
        "        modelo (str): O nome do modelo de embeddings.\n",
        "        arquitetura (str): A arquitetura específica do modelo.\n",
        "\n",
        "    Returns:\n",
        "        str: O caminho para o arquivo extraído.\n",
        "    \"\"\"\n",
        "    url = f\"http://143.107.183.175:22980/download.php?file=embeddings/{modelo}/{arquitetura}.zip\"\n",
        "    caminho_pasta_saida = os.path.join(\"embeddings\", modelo)\n",
        "    caminho_arquivo_saida = os.path.join(caminho_pasta_saida, arquitetura)\n",
        "    print(formata_msg(\"INFO\", f\"Baixando: {modelo}_{arquitetura}\"))\n",
        "    with urlopen(url) as resposta:\n",
        "        with ZipFile(BytesIO(resposta.read())) as arquivo_zip:\n",
        "            arquivo_zip.extractall(caminho_pasta_saida)\n",
        "\n",
        "    return os.path.join(caminho_pasta_saida, os.listdir(caminho_pasta_saida)[0])\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Funções utilitárias prontas para utilização.\"))\n",
        "print(formata_msg(\"INFO\", f\"Versão do Python: {sys.version} \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVzUjrWHNRs0",
        "outputId": "cf81e5cd-a4fb-4bfc-f490-31fa94f1c188"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-03-27 03:05:10 - Funções utilitárias prontas para utilização.\n",
            "[INFO] 2024-03-27 03:05:10 - Versão do Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2ZMw4zfAC3A"
      },
      "source": [
        "# Definição do Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxnmUk2NAC3M"
      },
      "source": [
        "Selecionamos algumas frases do corpus de BH da wikipedia.\n",
        "\n",
        "Conside a lista abaixo como nosso corpus de documentos. Cada elemento da lista, considere como um único documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd4pTiK0AC3M"
      },
      "source": [
        "documentos = \\\n",
        "[\"Belo Horizonte é um município brasileiro e a capital do estado de Minas Gerais\",\n",
        "\"A populacao de Belo Horizonte é estimada em 2 501 576 habitantes, conforme estimativas do Instituto Brasileiro de Geografia e Estatística\",\n",
        "\"Belo Horizonte já foi indicada pelo Population Crisis Commitee, da ONU, como a metrópole com melhor qualidade de vida na América Latina\",\n",
        "\"Belo Horizonte é mundialmente conhecida e exerce significativa influência nacional e até internacional, seja do ponto de vista cultural, econômico ou político\",\n",
        "\"Belo Horizonte é a capital do segundo estado mais populoso do Brasil, Minas Gerais\"]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1vjVVoYAC3T"
      },
      "source": [
        "## Preprocessamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_MuzVllAC3U"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "1) Escreva uma método que realiza o pré-processamento da lista de <b>documentos</b>.\n",
        "\n",
        "O método deve, para cada documento:\n",
        "- tokenizar cada palavra\n",
        "- remover stopwords\n",
        "- remover números\n",
        "- remover pontuções\n",
        "- remover acentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsfgYeVIa9PX"
      },
      "source": [
        "def pre_processa_texto(texto):\n",
        "    \"\"\"\n",
        "    Preprocessa o texto fornecido realizando várias etapas de limpeza.\n",
        "\n",
        "    Etapas:\n",
        "    1. Tokeniza o texto.\n",
        "    2. Converte os tokens para minúsculos.\n",
        "    3. Remove stopwords em português.\n",
        "    4. Remove números dos tokens.\n",
        "    5. Exclui tokens que são pontuações.\n",
        "    6. Remove acentuações dos tokens.\n",
        "\n",
        "    Parâmetros:\n",
        "    texto (str): O texto a ser preprocessado.\n",
        "\n",
        "    Retorna:\n",
        "    list: Lista de tokens preprocessados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokeniza o texto usando um padrão para capturar palavras e pontuações.\n",
        "    padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "    tokens_preprocessados = re.findall(padrao, texto)\n",
        "\n",
        "    # Converte os tokens para minúsculos para padronizar a capitalização.\n",
        "    tokens_preprocessados = [token.lower() for token in tokens_preprocessados]\n",
        "\n",
        "    # Remove stopwords para reduzir o conjunto de tokens a palavras significativas.\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in portugues_stops]\n",
        "\n",
        "    # Remove números, pois geralmente não contribuem para o significado do texto.\n",
        "    tokens_preprocessados = [re.sub(r'\\d+', '', token) for token in tokens_preprocessados\n",
        "                             if re.sub(r'\\d+', '', token)]\n",
        "\n",
        "    # Exclui tokens que são pontuações, pois raramente são úteis para análise de texto.\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in string.punctuation]\n",
        "\n",
        "    # Remove acentuações para padronizar os tokens.\n",
        "    tokens_preprocessados = [unidecode(token) for token in tokens_preprocessados]\n",
        "\n",
        "    return ' '.join(tokens_preprocessados)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_preprocessados = [pre_processa_texto(documento) for documento in documentos]"
      ],
      "metadata": {
        "id": "3FmIHc8XbK1T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9fbyAieAC3j"
      },
      "source": [
        "# Representação Textual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxy--9uzAC3x"
      },
      "source": [
        "### Phrases - Gensim\n",
        "\n",
        "Forma mais inteligente de calcular os bigrams. Ela calcula os bigramas levando em consideração a frequência do par das palavaras em todos os documentos.\n",
        "Para isso ele treina um modelo e depois aplica no corpus.\n",
        "\n",
        "```python\n",
        "#treinamento bigrams\n",
        "model_corpus_phrases = gensim.models.Phrases(corpus_processado, min_count=1)\n",
        "#calulando os bigrams do corpus processado\n",
        "bigram_corpus = model_corpus_phrases[corpus_processado]\n",
        "```\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "2) Faça um código que treine os bigrams, sendo que o <b>min_count = 1</b>.\n",
        "O <b>min_count</b> é a contagem mínima que aquele par de palavras deve aparecer junto para considerarmos com um token. Teste também com outros valores de mim_count. Depois imprima os bigramas de cada documento.\n",
        "Use o corpus_processado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JArQt0OPa_8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f553f3-d885-4c77-8bc5-1417681ea300"
      },
      "source": [
        "# treinamento bigrams\n",
        "model_corpus_phrases = gensim.models.Phrases(documentos_preprocessados, min_count=1)\n",
        "# calulando os bigrams do corpus processado\n",
        "bigram_corpus = model_corpus_phrases[documentos_preprocessados]\n",
        "print(bigram_corpus)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['belo horizonte municipio brasileiro capital estado minas gerais', 'populacao belo horizonte estimada habitantes conforme estimativas instituto brasileiro geografia estatistica', 'belo horizonte indicada population crisis commitee onu metropole melhor qualidade vida america latina', 'belo horizonte mundialmente conhecida exerce significativa influencia nacional internacional ponto vista cultural economico politico', 'belo horizonte capital segundo estado populoso brasil minas gerais']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFHut-GWAC4L"
      },
      "source": [
        "## TD-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZD0n75gAC4M"
      },
      "source": [
        "1) Faça o TDIFTVectorizer nos documentos da variável <b>documentos</b> sem alterar nenhum parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WMh8XBbClS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4c4d2e-304d-4b3d-d276-042477439b1f"
      },
      "source": [
        "# Inicializa o TfidfVectorizer com parâmetros específicos para\n",
        "# limitar a frequência máxima e mínima dos termos nos documentos.\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5)\n",
        "\n",
        "# Transforma os documentos em uma matriz TF-IDF.\n",
        "tfidf_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "# Imprime a forma da matriz TF-IDF para mostrar o número de documentos\n",
        "# e o tamanho do vocabulário.\n",
        "print(formata_msg(\"INFO\", f\"Forma da matriz TF-IDF: {tfidf_matrix.shape}\"))\n",
        "\n",
        "# Obtém o vocabulário extraído dos documentos e imprime.\n",
        "vocabulario = vectorizer.get_feature_names_out()\n",
        "print(formata_msg(\"INFO\", f\"Vocabulário:{vocabulario}\"))\n",
        "\n",
        "# Cria um DataFrame para os valores TF-IDF de cada termo por documento.\n",
        "df = pd.DataFrame(\n",
        "        tfidf_matrix.T.todense(),\n",
        "        index=vectorizer.get_feature_names_out(),\n",
        "        columns=[\"doc\" + str(i + 1) for i in range(len(documentos))]\n",
        "    )\n",
        "print(formata_msg(\"INFO\", f\"Dataframe:\\n\\n{df}\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-03-27 03:15:19 - Forma da matriz TF-IDF: (5, 5)\n",
            "[INFO] 2024-03-27 03:15:19 - Vocabulário:['brasileiro' 'capital' 'estado' 'gerais' 'minas']\n",
            "[INFO] 2024-03-27 03:15:19 - Dataframe:\n",
            "\n",
            "                doc1  doc2  doc3  doc4  doc5\n",
            "brasileiro  0.447214   1.0   0.0   0.0   0.0\n",
            "capital     0.447214   0.0   0.0   0.0   0.5\n",
            "estado      0.447214   0.0   0.0   0.0   0.5\n",
            "gerais      0.447214   0.0   0.0   0.0   0.5\n",
            "minas       0.447214   0.0   0.0   0.0   0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-ZevNeAC4-"
      },
      "source": [
        "## Bag of Words\n",
        "\n",
        "Faça o CountVectorizer nos documentos da variável <b>documentos</b> considerando binary = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z8_t3vabNMi"
      },
      "source": [
        "# Inicializa o TfidfVectorizer com parâmetros específicos para\n",
        "# limitar a frequência máxima e mínima dos termos nos documentos.\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "# Transforma os documentos em uma matriz TF-IDF.\n",
        "bow_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
        "\n",
        "# Imprime a forma da matriz TF-IDF para mostrar o número de documentos\n",
        "# e o tamanho do vocabulário.\n",
        "print(formata_msg(\"INFO\", f\"Forma da matriz TF-IDF: {bow_matrix.shape}\"))\n",
        "\n",
        "# Obtém o vocabulário extraído dos documentos e imprime.\n",
        "vocabulario = vectorizer.get_feature_names_out()\n",
        "print(formata_msg(\"INFO\", f\"Vocabulário:{vocabulario}\"))\n",
        "\n",
        "# Cria um DataFrame para os valores TF-IDF de cada termo por documento.\n",
        "df = pd.DataFrame(\n",
        "        bow_matrix.T.todense(),\n",
        "        index=vectorizer.get_feature_names_out(),\n",
        "        columns=[\"doc\" + str(i + 1) for i in range(len(documentos))]\n",
        "    )\n",
        "print(formata_msg(\"INFO\", f\"Dataframe:\\n\\n{df}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbLt5-LiAC5v"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nmucJX2AC5v"
      },
      "source": [
        "### Utilizando um embedding treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf94ykmVAC5w"
      },
      "source": [
        "9) Faça download do seguinte arquivo, realize a leitura deste arquivo e carregue o modelo do Repositório de Word Embeddings do NILC:\n",
        "http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "modelo, arquitetura=\"word2vec\", \"cbow_s50\"\n",
        "caminho_arquivo_completo = baixar_extrair(modelo, arquitetura )\n",
        "print(caminho_arquivo_completo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BksrOiokswjb",
        "outputId": "38a6a3c9-0ce0-4709-9bb7-d6c460247298"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 2024-03-27 03:16:59 - Baixando: word2vec_cbow_s50\n",
            "embeddings/word2vec/cbow_s50.txt\n",
            "CPU times: user 4.02 s, sys: 2.42 s, total: 6.44 s\n",
            "Wall time: 23.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5thOv6AAC50"
      },
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(caminho_arquivo_completo, binary=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7OvGRFIAC55"
      },
      "source": [
        "<b> Atividade </b>\n",
        "\n",
        "10) Imprima os vetores das palavras \"nlp\" e \"computacao\"\n",
        "\n",
        "```python\n",
        "#exemplo de retorno do vetor\n",
        "word_vectors[__]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_UyaYIUAC59"
      },
      "source": [
        "word_vectors.key_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWL_yba-bXIh"
      },
      "source": [
        "word_vectors['nlp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkeSO-BJAC6B"
      },
      "source": [
        "<b> Similaridade de Vetores </b>\n",
        "\n",
        "No gensim é possível realizar a similaridade utilizando o seguinte método:\n",
        "\n",
        "```python\n",
        "word_vectors.most_similar(___)\n",
        "```\n",
        "\n",
        "<b> Atividade </b>\n",
        "\n",
        "11) Verifique a similaridade das seguintes palavras: elizabete, raiva, segunda, dois, computação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzy1VhXcbcee"
      },
      "source": [
        "word_vectors.most_similar(\"elizabete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(\"raiva\")"
      ],
      "metadata": {
        "id": "rL6QTsBIwB_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(\"segunda\")"
      ],
      "metadata": {
        "id": "IswUgOx-wfaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(\"computação\")"
      ],
      "metadata": {
        "id": "stkZl7XYw80b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}