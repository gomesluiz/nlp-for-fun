{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesluiz/pln-na-pratica/blob/main/Pr%C3%A1tica_Pre_Processamento_Corrigido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ofFvi6aU2aT"
      },
      "outputs": [],
      "source": [
        "#Bibliotecas que precisam ser instaladas para a prática:\n",
        "!pip install unidecode==1.2.0\n",
        "!pip install wikipedia==1.4.0\n",
        "!pip install spacy==2.2.4\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxvjO-H9U2dt"
      },
      "source": [
        "# Técnicas de Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUfsnZq9U2du"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import wikipedia\n",
        "import re\n",
        "import spacy\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl3qVdoaU2dw"
      },
      "source": [
        "## Definindo o corpus\n",
        "\n",
        "Primeiramente, definimos o corpus que iremos trabalhar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LObmG0e_U2dx"
      },
      "outputs": [],
      "source": [
        "# Caso o pacote wikipedia não funcione:\n",
        "## Entre no link e faça downaload do arquivo https://drive.google.com/open?id=15R1jcugeM5SoGSPIPyG_6X6F3oMlxSBt\n",
        "## Depois leia com o comando abaixo\n",
        "#file = open(\"pln_wikipedia.txt\", 'r')\n",
        "#file_wiki = file.readlines()\n",
        "#file.close()\n",
        "#corpus = '\\n'.join(file_wiki)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN2IQNv5U2d1"
      },
      "outputs": [],
      "source": [
        "wikipedia.set_lang(\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM8TiMU4U2d4"
      },
      "outputs": [],
      "source": [
        "pln = wikipedia.page(\"PLN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hGJ6HMtU2d6"
      },
      "outputs": [],
      "source": [
        "corpus = pln.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmSdONIGU2d_"
      },
      "outputs": [],
      "source": [
        "print(\"O texto que estamos utilizando é da URL\",pln.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzO-lVIgU2eC"
      },
      "outputs": [],
      "source": [
        "print(pln.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdmSASmU2eF"
      },
      "source": [
        "## Tokenização\n",
        "\n",
        "Existem várias formas de realizar tokenização.\n",
        "1. Split()\n",
        "2. Regex\n",
        "3. NLTK\n",
        "4. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpC8C-NfU2eG"
      },
      "source": [
        "### Split\n",
        "\n",
        "Faça utilizando o método abaixo\n",
        "<b> Não esqueça de armazenar o resultado na variável abaixo. </b>\n",
        "\n",
        "```python\n",
        "str.split()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQWJf8eAU2eG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evq4lIQNU2eM"
      },
      "source": [
        "### Regex\n",
        "\n",
        "Faça utilizando o método abaixo\n",
        "<b> Não esqueça de armazenar o resultado na variável abaixo. </b>\n",
        "\n",
        "```python\n",
        "re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJOBqy-DU2eM"
      },
      "outputs": [],
      "source": [
        "tokens_regex ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWrk8Rm0U2eS"
      },
      "source": [
        "### NLTK\n",
        "\n",
        "Faça utilizando o método abaixo.\n",
        "<b> Não esqueça de armazenar o resultado na variável abaixo. </b>\n",
        "\n",
        "```python\n",
        "nltk.word_tokenize(corpus, language='portuguese')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHqORP70U2eT"
      },
      "outputs": [],
      "source": [
        "tokens_nltk ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xufxGJpPU2eX"
      },
      "outputs": [],
      "source": [
        "def plot_frequencia_tokens(tokens):\n",
        "    fd_words = FreqDist(tokens)\n",
        "    fd_words.plot(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdTpsjVU2eZ"
      },
      "source": [
        "### Plot das tokenizações\n",
        "\n",
        "Plote o resultado de cada uma das tokenizações.\n",
        "Utilize o método <b> plot_frequencia_tokens() </b> para plotar o gráfico.\n",
        "Também imprima o <b> tamanho de cada tokenização </b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7OOaS6Gew0m"
      },
      "outputs": [],
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IHVqfToexbE"
      },
      "outputs": [],
      "source": [
        "# Regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLyGZtmhezER"
      },
      "outputs": [],
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUOm1pgjU2ek"
      },
      "source": [
        "Como foi possível observar, existem variações nas tokenizações realizadas.\n",
        "A que gerou mais tokens foi a que utilizou regex.\n",
        "Por isso iremos utilizá-la a partir deste ponto.\n",
        "Utilize o resultado de <b> tokens_regex</b>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXN6naHeU2ek"
      },
      "source": [
        "## Capitalização\n",
        "\n",
        "traforme todos os tokens para minúsculo utilizando a função abaixo:\n",
        "\n",
        "```python\n",
        "str.lower()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgAOuq1sU2el"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT44nJ72U2en"
      },
      "source": [
        " <b> <span style=\"color:red\"> Lembre de utilizar a VARIÁVEL que contém o resultado da transformação anterior </span> </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBDXkVGU2en"
      },
      "source": [
        "## Remoção Stopwords\n",
        "\n",
        "remova todas as stopwords retornadas pelo pacote NLTK da lista de tokens.\n",
        "\n",
        "```python\n",
        "portugues_stops = stopwords.words('portuguese')\n",
        "```\n",
        "\n",
        "Note que a variável \"portugues_stops\" é uma lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vkUCUv2U2en"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vG30zABU2ew"
      },
      "outputs": [],
      "source": [
        "tokens_sem_stop ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpsE73A3U2ey"
      },
      "source": [
        "## Remoção Números\n",
        "\n",
        "remova todos os números, utilizando uma regex\n",
        "\n",
        "```python\n",
        "re.sub(__,__,__)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93QZ-mbOU2ez"
      },
      "outputs": [],
      "source": [
        "tokens_sem_numbers ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAziUGgxU2e0"
      },
      "source": [
        "## Remoção Pontuação\n",
        "\n",
        "remova todas as pontruações retornadas pelo pacote string da lista de tokens.\n",
        "\n",
        "```python\n",
        "string.punctuation\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JPCj-lXU2e6"
      },
      "outputs": [],
      "source": [
        "tokens_sem_punction ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM-c--fZU2fB"
      },
      "source": [
        "## Remoção Acentos\n",
        "\n",
        "remova todos os acentos utilizando a função abaixo:\n",
        "\n",
        "```python\n",
        "unidecode(str)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m2wSusXU2fB"
      },
      "outputs": [],
      "source": [
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1q1I-UhU2fD"
      },
      "outputs": [],
      "source": [
        "tokens_sem_acentos ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdD-DyW-U2fF"
      },
      "source": [
        "###  Plote a frequência dos tokens sem acentos. Utilize o método plot_frequencia_tokens() para plotar o gráfico. Também imprima o tamanho desta lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCyr8wlWU2fG"
      },
      "outputs": [],
      "source": [
        "# plot tokens_sem_acentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0B2VMc2U2fM"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "Iremos utilizar o stemming da biblioteca NLTK. O algoritmo disponível para este procedimento em portugês é o RSLPStemmer.\n",
        "\n",
        "```python\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "```\n",
        "\n",
        "Aplique o stemmet em cada elemento da lista de tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXsm_lS3U2fQ"
      },
      "outputs": [],
      "source": [
        "tokens_stemmer ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE9SWw5BU2fS"
      },
      "source": [
        "###  Plote a frequência dos tokens após o processo de stemming. Utilize o método plot_frequencia_tokens() para plotar o gráfico. Também imprima o tamanho desta lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJhUtTaIfMQS"
      },
      "outputs": [],
      "source": [
        "# plot tokens_stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRj0sT4oU2fV"
      },
      "source": [
        "## Lemmatization\n",
        "a biblitoteca NLTK não possui lematização para português.\n",
        "Mas a scpaCy possui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-oXoWovWrbt"
      },
      "outputs": [],
      "source": [
        "import pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFpQrNXQU2fV"
      },
      "outputs": [],
      "source": [
        "#carrega o modelo para português\n",
        "nlp = pt_core_news_sm.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBslXKwlU2fY"
      },
      "source": [
        "A primeira etapa para executar a lematização é transformar a lista de tokens para uma string.\n",
        "Utilize a variável de tokens: <b> tokens_sem_punction </b>.\n",
        "Você pode executar a <b> lematização </b> com acentos ou sem acentos. Funciona da mesma forma.\n",
        "\n",
        "Dica: utilize o método join para isto:\n",
        "\n",
        "```python\n",
        "str.join(list)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAo25jkFU2fY"
      },
      "outputs": [],
      "source": [
        "str_tokens = ' '.join(tokens_sem_punction)\n",
        "#str_tokens = ' '.join(tokens_sem_acentos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNTknXsLU2fb"
      },
      "source": [
        "Depois carregue a string de tokens (<b>str_tokens</b>) no modelo <b> nlp </b>, carregado em um dos passos anteriores.\n",
        "\n",
        "```python\n",
        "doc = nlp(str_tokens)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BchCPaZTU2fb"
      },
      "outputs": [],
      "source": [
        "doc ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uXzFRLWU2fd"
      },
      "source": [
        "Verifique o tipo da variável <b> doc </b>.\n",
        "Observe que é do tipo spacy.tokens.doc.Doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnxBJgCiU2fd"
      },
      "outputs": [],
      "source": [
        "type(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU495FKpU2ff"
      },
      "source": [
        "Como tipo da variável doc é do tipo spacy.tokens.doc.Doc.\n",
        "Apenas é preciso iterar em cada token e retornar o atributo <b> lemma_</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jOWhl2WU2fg"
      },
      "outputs": [],
      "source": [
        "tokens_lemm ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOUMZMmuU2fm"
      },
      "source": [
        "###  Plote a frequência dos tokens após o processo de lematização. Utilize o método plot_frequencia_tokens() para plotar o gráfico. Também imprima o tamanho desta lista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpgeBZnBfYU-"
      },
      "outputs": [],
      "source": [
        "# plot tokens_lemm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLXgdESYG-J"
      },
      "source": [
        "# Outras bibliotecas para pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFER3k8KYe-F"
      },
      "source": [
        "## Texhero\n",
        "\n",
        "https://texthero.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcyWSM2vYLW3"
      },
      "outputs": [],
      "source": [
        "!pip uninstall texthero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGATGrP5pzKo"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/jbesomi/texthero.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PUyIqqga_e4"
      },
      "outputs": [],
      "source": [
        "import texthero as hero\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQJUm1hNYn9w"
      },
      "outputs": [],
      "source": [
        "texto = \\\n",
        "\"\"\"A história      do PLN (Processamento de linguagem Natural) começou na década de 1950, quando Alan Turing publicou o artigo Computing Machinery and Intelligence.\n",
        "Fonte: https://pt.wikipedia.org/wiki/Processamento_de_linguagem_natural \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0AY96x7ZO9G"
      },
      "outputs": [],
      "source": [
        "texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM3NUA3UbFqR"
      },
      "outputs": [],
      "source": [
        "texto = pd.Series(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHHOhON2dMbm"
      },
      "outputs": [],
      "source": [
        "#Aplica tokenização\n",
        "hero.tokenize(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woEW8go1awYr"
      },
      "outputs": [],
      "source": [
        "#Remove numeros\n",
        "hero.remove_digits(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Snd94tbAKg"
      },
      "outputs": [],
      "source": [
        "#Remove pontuação\n",
        "hero.remove_punctuation(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_LnU83Nbqvi"
      },
      "outputs": [],
      "source": [
        "#Remove parênteses\n",
        "hero.remove_brackets(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4neqZ-_b2z4"
      },
      "outputs": [],
      "source": [
        "#Remove espaços em brancos\n",
        "hero.remove_whitespace(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnhY3alVbzV-"
      },
      "outputs": [],
      "source": [
        "#Remove stopwords\n",
        "portugues_stops = stopwords.words('portuguese')\n",
        "hero.remove_stopwords(texto, portugues_stops).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y0S3_GkcHoc"
      },
      "outputs": [],
      "source": [
        "#Remove URL\n",
        "hero.remove_urls(texto).values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXjdNzDrfqxF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWT1Es2jfrtz"
      },
      "source": [
        "## Clean-Text\n",
        "\n",
        "https://github.com/jfilter/clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjstQC5Ef75_",
        "outputId": "37f0f3c9-551a-49a3-e2e1-aa086eeb8dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clean-text==0.6.0\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0 (from clean-text==0.6.0)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text==0.6.0)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text==0.6.0) (0.2.13)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171034 sha256=debd44c33306c298cdb5459544a38177c8aa0962c726962fc965fbcd91ac9b73\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, ftfy, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DHK-hLigByp",
        "outputId": "c2e04465-d473-40e1-e559-e9e7c8c35b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ],
      "source": [
        "from cleantext import clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zf042_kqgFBW"
      },
      "outputs": [],
      "source": [
        "texto = \\\n",
        "\"\"\"A história      do PLN (Processamento de linguagem Natural) começou na década de 1950, quando Alan Turing publicou o artigo Computing Machinery and Intelligence.\n",
        "Fonte: https://pt.wikipedia.org/wiki/Processamento_de_linguagem_natural \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9LJ_H-Xtftge",
        "outputId": "1d1399f2-7700-4ab4-bae0-cdf682dcebf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a historia do pln processamento de linguagem natural comecou na decada de <number> quando alan turing publicou o artigo computing machinery and intelligence\\nfonte <url>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "clean(texto,\n",
        "    fix_unicode=True,\n",
        "    to_ascii=True,\n",
        "    lower=True,\n",
        "    no_line_breaks=False,\n",
        "    no_urls=True,\n",
        "    no_emails=False,\n",
        "    no_phone_numbers=False,\n",
        "    no_numbers=True,\n",
        "    no_digits=True,\n",
        "    no_currency_symbols=False,\n",
        "    no_punct=True,\n",
        "    replace_with_punct=\"\",\n",
        "    replace_with_url=\"<URL>\",\n",
        "    replace_with_email=\"<EMAIL>\",\n",
        "    replace_with_phone_number=\"<PHONE>\",\n",
        "    replace_with_number=\"<NUMBER>\",\n",
        "    replace_with_digit=\"0\",\n",
        "    replace_with_currency_symbol=\"<CUR>\",\n",
        "    lang=\"pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WdneJxXgJQJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wFER3k8KYe-F"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}