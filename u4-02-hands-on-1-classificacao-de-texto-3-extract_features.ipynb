{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesluiz/pln-na-pratica/blob/main/u4-02-hands-on-1-classificacao-de-texto-3-extract_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOdLlletf9z"
      },
      "source": [
        "# Bug Severity Predictor for Mozilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byFoszWXtf90"
      },
      "source": [
        "In this project, I'll build a severity predictor for the [Mozilla project](https://www.mozilla.org/en-US/) that uses the description of a bug report stored a in [Bugzilla Tracking System](https://bugzilla.mozilla.org/home) to predict its severity.\n",
        "\n",
        "The severity in the Mozilla project indicates how severe the problem is – from blocker (\"application unusable\") to trivial (\"minor cosmetic issue\"). Also, this field can be used to indicate whether a bug is an enhancement request. In my project, I have considered five severity levels: **trivial(0)**, **minor(1)**, **major(2)**, **critical(3)**, and **blocker(4)**. I have ignored the default severity level (often **\"normal\"**) because this level is considered as a choice made by users when they are not sure about the correct severity level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF3ekSWhtf91"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv8dCsKAtf92"
      },
      "source": [
        "This step in machine learning workflow will extract features from raw data via data mining techniques.These features can be used to improve the performance of\n",
        "machine learning algorithms. In my project, I'll use the [Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT)](https://arxiv.org/abs/1810.04805) to extract feature for my predicting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y23E3NBCtf92"
      },
      "source": [
        "## Project setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck0Ddykotf92"
      },
      "source": [
        "The cell below declares the required packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install nltk==3.8.1\n",
        "!pip install unidecode==1.3.8\n",
        "!pip install scikit-learn==1.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viCq1FEzrI8j",
        "outputId": "aebac7d0-a076-45bc-d988-6be9b132e500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.2)\n",
            "Requirement already satisfied: unidecode==1.3.8 in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYdhYgbFAMnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "5b6963c8-06fb-4617-e00b-d3bdef02e446"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'feature_engineering'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-855cc2a89dae>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_engineering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_features_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#from google.colab import drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#drive.mount('/drive')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feature_engineering'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from feature_engineering import extract_features_fn\n",
        "#from google.colab import drive\n",
        "#drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG0irGtWtf93"
      },
      "source": [
        "## Read in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r_MJmKztf93"
      },
      "source": [
        "The cell below load the cleaned bug reports dataset. This dataset has the following attributes:\n",
        "\n",
        "| **Attribute** | **Description** |\n",
        "| :------------ | :-------------- |\n",
        "| long_description |  The description of a report written when the bug report was opened. |\n",
        "| severity_code | The target label that represents the bug severity level.|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXw3-GAtAMoq"
      },
      "outputs": [],
      "source": [
        "batch_len=1000 # to preserve computational resources only 1000 bug reports were used.\n",
        "reports_input_path = os.path.join('..', 'data', 'clean')\n",
        "reports_data = pd.read_csv(os.path.join(reports_input_path, 'mozilla_bug_report_data.csv'))[:batch_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8gKQ7nfnAMow",
        "outputId": "d63a7f9c-e3b8-4433-e6e0-7f425dd2b127"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>long_description</th>\n",
              "      <th>severity_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is broken many users can t enter bugs on it p...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adding support for custom headers and cookie n...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the patch in bug regressed the fix from bug th...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from bugzilla helper user agent mozilla x u li...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i found it odd that relogin cgi didn t clear o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    long_description  severity_code\n",
              "0   is broken many users can t enter bugs on it p...              4\n",
              "1  adding support for custom headers and cookie n...              4\n",
              "2  the patch in bug regressed the fix from bug th...              2\n",
              "3  from bugzilla helper user agent mozilla x u li...              2\n",
              "4  i found it odd that relogin cgi didn t clear o...              1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reports_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pBJGXNUtf94"
      },
      "source": [
        "## Extracting features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDWwUNxtf94"
      },
      "source": [
        "These cells below extracts features from the dataset to be inputed in the model to predict the bug severity level. I've choose the [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5) as the feature extractor of my project.\n",
        "\n",
        "> \"DistilBERT processes the sentence and passes along some information it extracted from it on to the > next model. DistilBERT is a smaller version of [BERT](https://arxiv.org/abs/1810.04805) developed and open sourced by the\n",
        "> team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its\n",
        "> performance.\" (Jay Alammar in [A Visual Guide to Using BERT for the First Time](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processa_texto(texto):\n",
        "    \"\"\"\n",
        "    Preprocessa o texto fornecido realizando várias etapas de limpeza.\n",
        "\n",
        "    Etapas:\n",
        "    1. Tokeniza o texto.\n",
        "    2. Converte os tokens para minúsculos.\n",
        "    3. Remove stopwords em português.\n",
        "    4. Remove números dos tokens.\n",
        "    5. Exclui tokens que são pontuações.\n",
        "    6. Remove acentuações dos tokens.\n",
        "\n",
        "    Parâmetros:\n",
        "    texto (str): O texto a ser preprocessado.\n",
        "\n",
        "    Retorna:\n",
        "    list: Lista de tokens preprocessados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokeniza o texto usando um padrão para capturar palavras e pontuações.\n",
        "    padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "    tokens_preprocessados = re.findall(padrao, texto)\n",
        "\n",
        "    # Converte os tokens para minúsculos para padronizar a capitalização.\n",
        "    tokens_preprocessados = [token.lower() for token in tokens_preprocessados]\n",
        "\n",
        "    # Remove stopwords para reduzir o conjunto de tokens a palavras significativas.\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in portugues_stops]\n",
        "\n",
        "    # Remove números, pois geralmente não contribuem para o significado do texto.\n",
        "    tokens_preprocessados = [re.sub(r'\\d+', '', token) for token in tokens_preprocessados\n",
        "                             if re.sub(r'\\d+', '', token)]\n",
        "\n",
        "    # Exclui tokens que são pontuações, pois raramente são úteis para análise de texto.\n",
        "    tokens_preprocessados = [token for token in tokens_preprocessados\n",
        "                             if token not in string.punctuation]\n",
        "\n",
        "    # Remove acentuações para padronizar os tokens.\n",
        "    tokens_preprocessados = [unidecode(token) for token in tokens_preprocessados]\n",
        "\n",
        "    return ' '.join(tokens_preprocessados)\n"
      ],
      "metadata": {
        "id": "gH1PFIxorBLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbeFefCoAMo8"
      },
      "outputs": [],
      "source": [
        "# import pre-trained DistilBERT model and tokenizer\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n",
        "                                                    ppb.DistilBertTokenizer,\n",
        "                                                    'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLLEQtdbAMpA"
      },
      "outputs": [],
      "source": [
        "# load pretrained weigths in model/tokenizer objects.\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model     = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvzZLLbrtf95"
      },
      "outputs": [],
      "source": [
        "# extract features using the function extract_features_fn from feature_engineering\n",
        "# local package.\n",
        "features, labels  = extract_features_fn(reports_data, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOZA7rM5AMpi"
      },
      "outputs": [],
      "source": [
        "# split features and labels in training and testing sets.\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=.25\n",
        "                                                                            , stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6sgcjn8tf96"
      },
      "source": [
        "## Exporting the extracted features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRmDxZ9Ctf96"
      },
      "source": [
        "The cell below saves the training and testing sets in disk for training and testing steps\n",
        "machine workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f29jseiAMpk"
      },
      "outputs": [],
      "source": [
        "#processed_output_path = os.path.join('/','drive', 'My Drive', 'data', 'processed')\n",
        "processed_output_path = os.path.join('..','data', 'processed')\n",
        "if not os.path.exists(processed_output_path):\n",
        "    os.makedirs(processed_output_path)\n",
        "\n",
        "torch.save(np.column_stack((train_features, train_labels)),\n",
        "        os.path.join(processed_output_path, 'mozilla_bug_report_train_data.pt'))\n",
        "torch.save(np.column_stack((test_features, test_labels)),\n",
        "        os.path.join(processed_output_path, 'mozilla_bug_report_test_data.pt'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}