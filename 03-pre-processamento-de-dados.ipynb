{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "nlp-class",
      "language": "python",
      "name": "nlp-class"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wFER3k8KYe-F"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesluiz/pln-na-pratica/blob/main/03-pre-processamento-de-dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxvjO-H9U2dt"
      },
      "source": [
        "# Técnicas de Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ofFvi6aU2aT"
      },
      "source": [
        "# Bibliotecas que precisam ser instaladas para a prática:\n",
        "!pip install nltk==3.8.1\n",
        "!pip install spacy==3.7.2\n",
        "!pip install unidecode==1.3.8\n",
        "!pip install wikipedia==1.4.0\n",
        "!pip install matplotlib==3.7.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declara define funções utilitárias utilizadas no notebook.\n",
        "import datetime\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def formata_msg(nivel, msg):\n",
        "    \"\"\"\n",
        "    Formata uma mensagem de log incluindo o nível de severidade, timestamp\n",
        "    e a mensagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - nivel (str): Nível de severidade da mensagem (ex: 'INFO', 'ERROR', 'WARNING').\n",
        "    - msg (str): A mensagem de log propriamente dita.\n",
        "\n",
        "    Retorna:\n",
        "    - str: A mensagem de log formatada.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    return f\"[{nivel}] {timestamp} - {msg}\"\n",
        "\n",
        "\n",
        "def calcula_pct_reducao(valor_inicial, valor_final):\n",
        "  \"\"\"\n",
        "  Calcula a porcentagem de redução de um valor inicial para um valor final.\n",
        "\n",
        "  Args:\n",
        "      valor_inicial (float): O valor inicial antes da redução.\n",
        "      valor_final (float): O valor após a redução.\n",
        "\n",
        "  Returns:\n",
        "      float: A porcentagem de redução do valor inicial para o valor final.\n",
        "  \"\"\"\n",
        "  return (valor_inicial - valor_final)/valor_inicial * 100\n",
        "\n",
        "\n",
        "def plot_frequencia_tokens(tokens, ax, title):\n",
        "    \"\"\"\n",
        "    Plota a frequência dos 20 tokens mais comuns em um gráfico de barras horizontais,\n",
        "    formatado de acordo com os princípios de storytelling.\n",
        "\n",
        "    Gera um gráfico de barras horizontais no objeto de eixo do Matplotlib fornecido,\n",
        "    destacando os tokens mais comuns e suas frequências, com foco em clareza e\n",
        "    comunicação eficaz dos dados.\n",
        "\n",
        "    Parâmetros:\n",
        "    - tokens (list): Lista de tokens para análise de frequência.\n",
        "    - ax (matplotlib.axes.Axes): Eixo onde o gráfico será plotado.\n",
        "    - title (str, opcional): Título do gráfico.\n",
        "\n",
        "    Retorna:\n",
        "    - None\n",
        "\n",
        "    Exemplo:\n",
        "    >>> fig, ax = plt.subplots()\n",
        "    >>> plot_frequencia_tokens(['a', 'b', 'a'], ax)\n",
        "    \"\"\"\n",
        "    fd_words = FreqDist(tokens)\n",
        "    most_common = fd_words.most_common(20)\n",
        "    words, frequencies = zip(*most_common)\n",
        "    y_pos = range(len(words))\n",
        "\n",
        "    ax.barh(y_pos, frequencies, align='center', color='skyblue')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(words, fontsize=10)\n",
        "    ax.invert_yaxis()  # Inverte o eixo y para que os valores mais altos fiquem no topo\n",
        "\n",
        "    for i, v in enumerate(frequencies):\n",
        "        ax.text(v + 0.1, i, str(v), color='black', va='center', fontsize=8)\n",
        "\n",
        "    ax.set_xlabel('Frequência', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.6)  # Adiciona uma grade sutil no eixo x\n",
        "\n",
        "    # Remove as bordas superior e direita para um visual mais limpo\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Funções utilitárias prontas para utilização.\"))\n",
        "print(formata_msg(\"INFO\", f\"Versão do Python: {sys.version} \"))"
      ],
      "metadata": {
        "id": "B-AqP9oydVfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUfsnZq9U2du"
      },
      "source": [
        "# Importa módulos essenciais para funcionalidades do notebook.\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "from unidecode import unidecode\n",
        "import wikipedia\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Pacotes importadas com sucesso; ambiente pronto.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixa as bibliotecas necessárias do NLTK e do Spacy\n",
        "nltk.download(\"stopwords\")        # Baixa lista de stopwords\n",
        "nltk.download('punkt')            # Baixa dados para o tokenizador\n",
        "nltk.download('rslp')             # Baixa o stemmer RSLP para o português\n",
        "!spacy download pt_core_news_sm   # Baixa o modelo de linguagem pré-treinado fornecido pela biblioteca spaCy\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Download das bases do NLTK e Spacy realizado com sucesso.\"))"
      ],
      "metadata": {
        "id": "5jPd2NnheC7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl3qVdoaU2dw"
      },
      "source": [
        "## Wikipedia\n",
        "\n",
        "A Wikipedia é uma enciclopédia digital gratuita e de código aberto que permite aos usuários ler e editar seu conteúdo. Representando a maior e mais popular fonte de referência na internet, destaca-se pelo seu modelo colaborativo, no qual voluntários de todo o mundo contribuem com informações em vários idiomas. Para acessar e interagir com o conteúdo da Wikipedia usando Python, pode-se utilizar o pacote \"wikipedia\", uma biblioteca Python desenvolvida para simplificar a busca e recuperação de artigos, além de facilitar a obtenção de resumos, links e outras informações disponíveis na Wikipedia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN2IQNv5U2d1"
      },
      "source": [
        "# Define o idioma da Wikipedia para Português\n",
        "wikipedia.set_lang(\"pt\")\n",
        "\n",
        "# Busca a página da Wikipedia sobre Processamento de Linguagem Natural (PLN)\n",
        "pln = wikipedia.page(\"PLN\")\n",
        "\n",
        "# Extrai o conteúdo da página como um corpus de texto\n",
        "corpus = pln.content\n",
        "\n",
        "print(formata_msg(\"INFO\",f\"1000 caracteres do conteúdo textual da URL {pln.url}:\\n\"))\n",
        "print(\"--- INÍCIO ---\")\n",
        "print(pln.content[0:1000])\n",
        "print(\"--- TÉRMINO ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QdmSASmU2eF"
      },
      "source": [
        "## 1. Tokenização\n",
        "A tokenização é o processo inicial de pré-processamento de textos, que consiste na divisão do conteúdo em unidades menores, como palavras, termos ou tokens. Essas unidades podem incluir palavras, números e sinais de pontuação, essenciais para a compreensão e análise de textos em linguagem natural. A identificação precisa desses elementos é crucial para o processamento eficaz do texto. Existem diversas técnicas para realizar a tokenização, incluindo:\n",
        "\n",
        "* Utilização da função split() para separar o texto em tokens baseando-se em espaços ou outros delimitadores.\n",
        "* Uso de expressões regulares (Regex) para uma tokenização mais detalhada e flexível.\n",
        "* Emprego da biblioteca Natural Language Toolkit (NLTK), especializada em processamento de linguagem natural, que fornece ferramentas avançadas para tokenização, entre outras funções de processamento de texto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpC8C-NfU2eG"
      },
      "source": [
        "### 1.1. Método **string.split()** da biblioteca padrão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQWJf8eAU2eG"
      },
      "source": [
        "# Divide o corpus em tokens e armazena na lista\n",
        "tokens_split = corpus.split()\n",
        "\n",
        "# Exibe os tokens e o número total de tokens\n",
        "print(formata_msg(\"INFO\", f\"Tokens com o string.split(): \\n{tokens_split}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens com o string.split(): {len(tokens_split)}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evq4lIQNU2eM"
      },
      "source": [
        "### 1.2. Método **re.findall()** da biblioteca padrão\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJOBqy-DU2eM"
      },
      "source": [
        "# Padrão regular para capturar tokens:\n",
        "    # \\w+          - Sequência de caracteres alfanuméricos/underscore\n",
        "    # (?:'\\w+)?    - Opcional: apóstrofo seguido de caracteres alfanuméricos\n",
        "    # |            - OU\n",
        "    # [^\\w\\s]      - Um único caractere que não é alfanumérico ou espaço em branco\n",
        "padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
        "\n",
        "# Utiliza a expressão regular para encontrar todos os tokens no corpus\n",
        "tokens_regex = re.findall(padrao, corpus)\n",
        "\n",
        "print(formata_msg(\"INFO\", f\"Tokens com o re.findall(): \\n{tokens_regex}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho: {len(tokens_regex)}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWrk8Rm0U2eS"
      },
      "source": [
        "### 1.3. Método **nltk.word_tokenize()**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHqORP70U2eT"
      },
      "source": [
        "# Tokeniza o corpus utilizando a função word_tokenize do NLTK\n",
        "# Configura o idioma para português\n",
        "tokens_nltk = nltk.word_tokenize(corpus, language='portuguese')\n",
        "\n",
        "# Exibe os tokens extraídos e a quantidade total\n",
        "print(formata_msg(\"INFO\", f\"Tokens com nltk.word_tokenize(): \\n{tokens_nltk}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens nltk.word_tokenize(): {len(tokens_nltk)}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdTpsjVU2eZ"
      },
      "source": [
        "### 1.4. Gráfico de frequências de tokens por método de tokenização\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma figura e três subplots\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))  # Ajuste o tamanho conforme necessário\n",
        "\n",
        "# Plotando as frequências para cada lista de tokens em um subplot diferente\n",
        "plot_frequencia_tokens(tokens_split, axs[0], f\"Top 20 tokens mais comuns\\nsplit\")\n",
        "plot_frequencia_tokens(tokens_regex, axs[1], f\"Top 20 tokens mais comuns\\nregex\")\n",
        "plot_frequencia_tokens(tokens_nltk, axs[2], f\"Top 20 tokens mais comuns\\nnltk\")\n",
        "\n",
        "plt.tight_layout()  # Ajusta automaticamente os parâmetros do subplot\n",
        "plt.show()  # Mostra a figura com os três gráficos lado a lado"
      ],
      "metadata": {
        "id": "zESP8c4CP4O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUOm1pgjU2ek"
      },
      "source": [
        "Como observado, há variações nas tokenizações realizadas, sendo a que utilizou expressões regulares (regex) a que gerou mais tokens. Portanto, vamos utilizar essa abordagem a partir de agora, com os resultados obtidos por **tokens_regex**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXN6naHeU2ek"
      },
      "source": [
        "## 2. Capitalização com **str.lower()**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgAOuq1sU2el"
      },
      "source": [
        "# Utiliza uma compreensão de lista para converter cada token para minúsculo\n",
        "tokens_minusculos = [token.lower() for token in tokens_regex]\n",
        "\n",
        "# Exibe os tokens convertidos\n",
        "print(formata_msg(\"INFO\", f\"Tokens em minúsculos: \\n{tokens_minusculos}\\n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBDXkVGU2en"
      },
      "source": [
        "## 3. Remoção stopwords com a NLTK\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vG30zABU2ew"
      },
      "source": [
        "# Carrega a lista de stopwords para o idioma especificado.\n",
        "portugues_stops = stopwords.words('portuguese')\n",
        "\n",
        "# Filtra os tokens, removendo aqueles que são stopwords.\n",
        "tokens_sem_stop = [token for token in tokens_regex_minusculos if token not in portugues_stops]\n",
        "\n",
        "# Exibe os tokens extraídos e a quantidade total\n",
        "print(formata_msg(\"INFO\", f\"Tokens sem stopwords: \\n{tokens_sem_stop}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens sem stopwords: {len(tokens_sem_stop)}\"))\n",
        "print(formata_msg(\"INFO\", f\"Redução de tokens em relação à quantidade anterior em: {calcula_pct_reducao(len(tokens_regex_minusculos), len(tokens_sem_stop)):.2f}%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpsE73A3U2ey"
      },
      "source": [
        "## 4. Remoção números com regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93QZ-mbOU2ez"
      },
      "source": [
        "# Filtra os tokens, removendo aqueles que são números.\n",
        "tokens_sem_numeros = [re.sub(r'\\d+', '', palavra) for palavra in tokens_sem_stop if re.sub(r'\\d+', '', palavra)]\n",
        "\n",
        "# Exibe os tokens extraídos e a quantidade total\n",
        "print(formata_msg(\"INFO\", f\"Tokens sem números: \\n{tokens_sem_numeros}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens sem números: {len(tokens_sem_numeros)}\"))\n",
        "print(formata_msg(\"INFO\", f\"Redução de tokens em relação à quantidade anterior em: {calcula_pct_reducao(len(tokens_sem_stop), len(tokens_sem_numeros)):.2f}%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAziUGgxU2e0"
      },
      "source": [
        "## 5. Remoção pontuação com **string.punctuation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JPCj-lXU2e6"
      },
      "source": [
        "# Filtra os tokens, removendo aqueles que são pontuações.\n",
        "tokens_sem_pontuacao = [token for  token in tokens_sem_numeros if token not in string.punctuation]\n",
        "\n",
        "print(formata_msg(\"INFO\", f\"Tokens sem pontuações: \\n{tokens_sem_pontuacao}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens sem pontuacoes: {len(tokens_sem_pontuacao)}\"))\n",
        "print(formata_msg(\"INFO\", f\"Redução de tokens em relação à quantidade anterior em: {calcula_pct_reducao(len(tokens_sem_numeros), len(tokens_sem_pontuacao)):.2f}%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM-c--fZU2fB"
      },
      "source": [
        "## 6. Remoção acentos com o **unidecode(str)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1q1I-UhU2fD"
      },
      "source": [
        "tokens_sem_acentos = [unidecode(token) for token in tokens_sem_pontuacao]\n",
        "\n",
        "print(formata_msg(\"INFO\", f\"Tokens sem acentos: \\n{tokens_sem_acentos}\\n\"))\n",
        "print(formata_msg(\"INFO\", f\"Tamanho dos tokens sem acentos: {len(tokens_sem_pontuacao)}\"))\n",
        "print(formata_msg(\"INFO\", f\"Redução de tokens em relação à quantidade anterior em: {calcula_pct_reducao(len(tokens_sem_pontuacao), len(tokens_sem_acentos)):.2f}%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdD-DyW-U2fF"
      },
      "source": [
        "### 6.1. Gráfico de frequência dos tokens sem acentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCyr8wlWU2fG"
      },
      "source": [
        "# Criando uma figura e três subplots\n",
        "fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Ajuste o tamanho conforme necessário\n",
        "\n",
        "# Plotando as frequências para cada lista de tokens em um subplot diferente\n",
        "plot_frequencia_tokens(tokens_sem_acentos, axs, f\"Top 20 tokens mais comuns\\napós a remoção dos acentos\")\n",
        "\n",
        "plt.tight_layout()  # Ajusta automaticamente os parâmetros do subplot\n",
        "plt.show()  # Mostra a figura com os três gráficos lado a lado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0B2VMc2U2fM"
      },
      "source": [
        "## 8. Stemming com a NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXsm_lS3U2fQ"
      },
      "source": [
        "# Inicialize o stemmer RSLPStemmer do NLTK\n",
        "stemmed = nltk.stem.RSLPStemmer()\n",
        "\n",
        "# Aplique o stemmer a cada token da lista, gerando uma nova lista de tokens 'stemmed'\n",
        "tokens_stemmed = [stemmed.stem(token) for token in tokens_sem_acentos]\n",
        "\n",
        "print(formata_msg(\"INFO\", f\"Tokens sem acentos: \\n{tokens_stemmed}\\n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE9SWw5BU2fS"
      },
      "source": [
        "### 8.1. Plote a frequência dos tokens após o processo de stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJhUtTaIfMQS"
      },
      "source": [
        "# Criando uma figura e três subplots\n",
        "fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Ajuste o tamanho conforme necessário\n",
        "\n",
        "# Plotando as frequências para cada lista de tokens em um subplot diferente\n",
        "plot_frequencia_tokens(tokens_stemmed, axs, f\"Top 20 tokens mais comuns\\napós a remoção dos acentos\")\n",
        "\n",
        "plt.tight_layout()  # Ajusta automaticamente os parâmetros do subplot\n",
        "plt.show()  # Mostra a figura com os três gráficos lado a lado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRj0sT4oU2fV"
      },
      "source": [
        "## 9. Lemmatization com o Spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFpQrNXQU2fV"
      },
      "source": [
        "# Carrega o modelo de lematização para português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAo25jkFU2fY"
      },
      "source": [
        "# Junte os tokens em uma única string, separada por espaços.\n",
        "str_tokens = ' '.join(tokens_sem_pontuacao)\n",
        "\n",
        "# Processa a string de tokens com o modelo de linguagem natural.\n",
        "doc = nlp(str_tokens)\n",
        "\n",
        "# Gera os lemas dos tokens processados.\n",
        "tokens_lemm = [token.lemma_ for token in doc]\n",
        "\n",
        "\n",
        "print(formata_msg(\"INFO\", f\"Tokens lematizados: \\n{tokens_lemm}\\n\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOUMZMmuU2fm"
      },
      "source": [
        "### 9.1. Plote a frequência dos tokens após o processo de stemming.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpgeBZnBfYU-"
      },
      "source": [
        "# Criando uma figura e três subplots\n",
        "fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Ajuste o tamanho conforme necessário\n",
        "\n",
        "# Plotando as frequências para cada lista de tokens em um subplot diferente\n",
        "plot_frequencia_tokens(tokens_lemm, axs, f\"Top 20 tokens mais comuns\\napós a lematização\")\n",
        "\n",
        "plt.tight_layout()  # Ajusta automaticamente os parâmetros do subplot\n",
        "plt.show()  # Mostra a figura com os três gráficos lado a lado"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}