{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCwpHd6uIrc0gRm8tZhMdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesluiz/pln-na-pratica/blob/main/01-aquisicao-dados-enriquecimento-de-dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aquisição de dados com raspagem de dados**"
      ],
      "metadata": {
        "id": "R3tUTgb_kpwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação dos pacotes necessários.\n",
        "!pip install beautifulsoup4 pandas requests"
      ],
      "metadata": {
        "id": "NB3Mmh26EKbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de pacotes básicos para a execução deste notebook.\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Pacotes importados com sucesso.\")"
      ],
      "metadata": {
        "id": "8ueIJQz8n-h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o diretório local para armazenar as bases de dados públicas coletadas.\n",
        "corpora_caminho = \"./corpora\"\n",
        "\n",
        "# Verifica se o diretório especificado já existe no ambiente do Colab.\n",
        "if not os.path.exists(corpora_caminho):\n",
        "  # Caso o diretório não exista, cria um novo diretório com o nome 'corpora'.\n",
        "  # Isso é útil para organizar os arquivos de dados baixados ou gerados.\n",
        "  os.mkdir(corpora_caminho)\n",
        "\n",
        "print(f\"Diretório {corpora_caminho} criado com sucesso.\")\n"
      ],
      "metadata": {
        "id": "Ld0Thaj9nqzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. G1"
      ],
      "metadata": {
        "id": "pzFkGfrRd65P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raspagem de dados de notícias do G1"
      ],
      "metadata": {
        "id": "q6aSu0KT4paP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O g1 é um portal de notícias brasileiro mantido pelo Grupo Globo e sob orientação da Central Globo de Jornalismo. Foi lançado em 18 de setembro de 2006, ano que a TV Globo fez 41 anos. O portal disponibiliza o conteúdo de jornalismo das diversas empresas do Grupo Globo - TV Globo, GloboNews, rádio CBN, Jornais O Globo, Extra e Valor Econômico, revista e Globo Rural, entre outras - além de reportagens próprias em formato de texto, fotos, áudio e vídeo. Além das cinco redações próprias situadas no Rio de Janeiro, em São Paulo, Brasília, Belo Horizonte e Recife, afiliadas da TV Globo, jornais, revistas, rádios e as agências de Notícias como Agência Estado, Agência France Presse, Associated Press, EFE, New York Times, Lusa, Reuters e Valor Econômico alimentam o portal de notícias, que é atualizado 24 horas por dia."
      ],
      "metadata": {
        "id": "c0tgNZXdnG07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVXqB01uDacX"
      },
      "outputs": [],
      "source": [
        "# Define a URL da base pública armazenada no Github.\n",
        "url = \"https://g1.globo.com/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do arquivo para armazenamento da base de dados.\n",
        "noticias_arquivo = \"g1_noticias.csv\"\n",
        "\n",
        "# O método os.path.join cria o caminho completo para o arquivo, combinando\n",
        "# 'corpora_caminho' com o nome do arquivo 'b2w_arquivo'.\n",
        "noticias_caminho = os.path.join(corpora_caminho, noticias_arquivo)\n",
        "\n",
        "# Executa o download do arquivo especificado na URL com os parâmetros\n",
        "# especificados\n",
        "noticias_g1 = []\n",
        "\n",
        "resposta = requests.get(url)\n",
        "if resposta.status_code == 200:\n",
        "  # Se o status for igual a 200 (sucesso), grava o arquivo no caminho\n",
        "  # especificado\n",
        "  pagina = resposta.content\n",
        "  soup = BeautifulSoup(pagina, 'html.parser')\n",
        "  noticias = soup.findAll('div', attrs={'class': 'feed-post-body'})\n",
        "\n",
        "  for noticia in noticias:\n",
        "    # Título\n",
        "    titulo = noticia.find('a', attrs={'class': 'feed-post-link'})\n",
        "\n",
        "    # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "    subtitulo = noticia.find('div', attrs={'class': 'feed-post-body-resumo'})\n",
        "\n",
        "    if (subtitulo):\n",
        "      noticias_g1.append([titulo.text, subtitulo.text, titulo['href']])\n",
        "    else:\n",
        "      noticias_g1.append([titulo.text, '', titulo['href']])\n",
        "\n",
        "  noticias_g1_df = pd.DataFrame(noticias_g1, columns=['Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "  noticias_g1_df.to_csv(noticias_caminho, index=False)\n",
        "\n",
        "  print(f\"Raspagem de dados da URL {url} gravada em {noticias_arquivo} com sucesso!\")\n",
        "else:\n",
        "    print(\"Falha na solicitação, código de status:\", resposta.status_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "woasdVNlm_gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O comando head do linux mostra as cinco primeiras linhas do arquivo gravado.\n",
        "noticias_g1_df = pd.read_csv(noticias_caminho)\n",
        "noticias_g1_df.head(10)"
      ],
      "metadata": {
        "id": "57kIS4iOsTM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}